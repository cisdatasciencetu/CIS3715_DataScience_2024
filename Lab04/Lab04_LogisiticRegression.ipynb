{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will learn the Logistic Regression model.\n",
    "\n",
    "First, please study the given example, which uses the logistic regression model for the breast cancer classification task. In this example, you will learn how to preprocess data, how to train the model, and how to evaluate the model.\n",
    "\n",
    "Based on the given example, your task is to use the logistic regression model to predict the presence of heart disease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the breast cancer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the [breast cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) dataset in sklearn. It is a binary classification dataset. Each sample has 30 numerical features, which can be found in [7.1.7](https://scikit-learn.org/stable/datasets/toy_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 569, #features: 30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "print(\"#samples: {}, #features: {}\".format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Split the data into two subsets and normalize the features of samples\n",
    "\n",
    "Here, we use 69 samples as the testing set and use the remained samples to train the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val: 500, test: 69\n"
     ]
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
    "                                                            test_size=0.12, \n",
    "                                                            random_state=0)\n",
    "print(\"train_val: {}, test: {}\".format(X_train_val.shape[0], X_test.shape[0]))\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_val = normalizer.fit_transform(X_train_val)\n",
    "X_test = normalizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train the logistic regression model and select the hyperparameter with cross-validation\n",
    "\n",
    "Here, we use the following logistic regression model to do cancer classification. \n",
    "\n",
    "\\begin{equation}\n",
    "\t\\min_{\\mathbf{w}} \\sum_{i=1}^{n}\\{\\log(1+\\exp(\\mathbf{w}^T\\mathbf{x}_i))-y_i\\mathbf{w}^T\\mathbf{x}_i \\} + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "We need to learn the model parameter $\\mathbf{w}$. However, with different hyperparameters $\\lambda$, we can get different model parameter $\\mathbf{w}$, resulting in different prediction performance. Here, we use the 5-fold cross-validation to select the hyperparameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[380 468 145   2 282 103 148 312 130 411 168 204 113 132 418 270 351 157\n",
      "  451 339 288 277 354  48 318 303 314 234  95 304 271 434 173 357 495 133\n",
      "  431  39 490 310 317 471  23 426 224 286  20 365 255 216 405  79 228 445\n",
      "  189 184 243 358 276 218 488  60 438 159 167 349  89 121 333  51   9 482\n",
      "  152 416 379 306 111 185 340 489 475  93  84 376 291 158 250 323 406 460\n",
      "   50 433 372  66 108 465  71 298 369 437]\n",
      " [211  11 110 142  28  59 163  38  24 205 440 140 177 252 235 245 242  25\n",
      "   21 217 160 231  77 151  54 345 280 257 456 308 331  58 360 179 464 388\n",
      "  129 285 347  56 387 169  36 138 319 296 246 122  33 127 109 363 183 196\n",
      "  422  86 400 297 346 116  63  88 477 144 112 362 399 334  62 353 146 373\n",
      "   27  76 260 150 210 195 290  82 154 432 320 361  75  17  94 238 143 469\n",
      "   67 225 391 106  15  97  46  49 192 226]\n",
      " [114 302 356  91  80 107 329 209 384 409  13 176 299 483 295 491 332 292\n",
      "  153 202 268   1 417 313 375 128 352  57 408 254 382 390 377 328 213 182\n",
      "   65   7 315 101 187 126 123 394 201 251 494 239 383 367 237  34 307 141\n",
      "  403 344 162  43 118 498  99 392 102 258 100  41 281 364 492 448 164 104\n",
      "  124 259 355 458 484 115 309 338  53 381 442  70 284 263 419 166 441 481\n",
      "  335 219 155 294 230 378 476 232 480  31]\n",
      " [343 197 301  85  61 264 446 273 455 188 199 452  74 443 423 395 265  29\n",
      "   40 120 190  73 348 415 474 337  12 178 212 402 478 412 241 454 165  14\n",
      "  206 325 279 398 366 462   4 221 421 389 181 413  32 316 493 473 215 324\n",
      "  425 139 424 385 131 453  98 470  68   5 459 236 466 227 487  78  90 439\n",
      "  278 119 368 322 253 147 435  30 397 256 272 207 117 180 430 186 321  45\n",
      "  300  96   8 401 450 198 233 370  37 200]\n",
      " [283 479 171  87 134 336 249  42 371  92 427 386  16 261 191 214 342 266\n",
      "  248 467 457 407 326 275 350 222 262 330 444 203   6 472 414 289 269 327\n",
      "  311 420 105 247 410 267 175 156 496  18 428 240 135 244 293 220 149  10\n",
      "  404  64  72 341  47  22  52 229 374 161   3  35 193 305 449 497 396 223\n",
      "  463   0  83 125 359 485 486 172  69  81 499 436 174 170 287 274 194  19\n",
      "  447 461 429  55 136 208 393  44 137  26]]\n",
      "reg_coeff: 10.0, acc: 0.970\n",
      "reg_coeff: 2.0, acc: 0.978\n",
      "reg_coeff: 1.0, acc: 0.972\n",
      "reg_coeff: 0.2, acc: 0.968\n",
      "reg_coeff: 0.1, acc: 0.968\n"
     ]
    }
   ],
   "source": [
    "# here we use 5-fold cross-validation\n",
    "folds = 5\n",
    "\n",
    "# get the number of samples in the training and validation set\n",
    "num_train_val = X_train_val.shape[0] \n",
    "\n",
    "# shuffle the index of samples in the train_val set\n",
    "index_of_samples = np.arange(num_train_val) \n",
    "shuffle(index_of_samples)\n",
    "\n",
    "# split the index of the train_valid set into 5 folds\n",
    "index_of_folds = index_of_samples.reshape(folds, -1)\n",
    "print(index_of_folds)\n",
    "\n",
    "# potential hyperparameters. \n",
    "#These hyperparameters are just used for illustration. \n",
    "#You should try more hyperparameters to get a good model.\n",
    "#The hyperparameters must be nonnegative!\n",
    "regularization_coefficient = [0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for reg in regularization_coefficient:\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    sum_acc = 0.0\n",
    "    for fold in range(folds):\n",
    "        \n",
    "        index_of_folds_temp = index_of_folds.copy()\n",
    "        \n",
    "        valid_index = index_of_folds_temp[fold,:].reshape(-1) #get the index of the validation set\n",
    "        train_index = np.delete(index_of_folds_temp, fold, 0).reshape(-1) #get the index of the training set\n",
    "        \n",
    "        # training set\n",
    "        X_train = X_train_val[train_index]\n",
    "        y_train = y_train_val[train_index]\n",
    "        \n",
    "        # validation set\n",
    "        X_valid = X_train_val[valid_index]\n",
    "        y_valid = y_train_val[valid_index]\n",
    "                \n",
    "        # build the model with different hyperparameters\n",
    "        clf = LogisticRegression(penalty='l2', C=reg, solver='lbfgs')\n",
    "        \n",
    "        #train the model with the training set\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_valid_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(y_valid, y_valid_pred)\n",
    "        \n",
    "        sum_acc += acc\n",
    "    \n",
    "    cur_acc = sum_acc / folds\n",
    "    \n",
    "    print(\"reg_coeff: {}, acc: {:.3f}\".format(1.0/reg, cur_acc))\n",
    "    \n",
    "    # store the best hyperparameter\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_reg = reg\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Evaluate the learned model\n",
    "\n",
    "After getting the best hyperparameter $\\lambda$, we retrain the model with the train_val set. Then, we evaluate this  model on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000, recall: 1.000, precision: 1.000, f1: 1.000,\n"
     ]
    }
   ],
   "source": [
    "# retrain the model\n",
    "clf = LogisticRegression(penalty='l2', C=best_reg, solver='lbfgs')\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(\"accuracy: {:.3f}, recall: {:.3f}, precision: {:.3f}, f1: {:.3f},\".format(acc, recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task\n",
    "\n",
    "Here, we use the [heart disease](./heart.csv) dataset. Each sample has the following feature: \n",
    "\n",
    "* age\n",
    "* sex\n",
    "* chest pain type (4 values)\n",
    "* resting blood pressure\n",
    "* serum cholestoral in mg/dl\n",
    "* fasting blood sugar > 120 mg/dl\n",
    "* resting electrocardiographic results (values 0,1,2)\n",
    "* maximum heart rate achieved\n",
    "* exercise induced angina\n",
    "* oldpeak = ST depression induced by exercise relative to rest\n",
    "* the slope of the peak exercise ST segment\n",
    "* number of major vessels (0-3) colored by flourosopy\n",
    "* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "The last column refers to the presence of heart disease in the patient.\n",
    "\n",
    "The task is to predict whether a person has the heart disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess the raw data\n",
    "\n",
    "* Check whether there are missing values\n",
    "* Check whether theare are cateogrical features\n",
    "* Check whether this dataset is balanced or not (use the bar plot to visualize the number of positive and negative samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split the data into two subsets and normalize the features of samples\n",
    "\n",
    "* Split the dataset into the train_val set and testing set. \n",
    "* Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the logistic regression model and select the hyperparameter with cross-validation\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\min_{\\mathbf{w}} \\sum_{i=1}^{n}\\{\\log(1+\\exp(\\mathbf{w}^T\\mathbf{x}_i))-y_i\\mathbf{w}^T\\mathbf{x}_i \\} + \\lambda\\|\\mathbf{w}\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "* Use the 10-fold cross-validation to select the hyperparameter $\\lambda$.\n",
    "* Search $\\lambda$ from $\\{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 20, 50, 100\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate the learned model\n",
    "\n",
    "* Report the prediction accuracy, recall, precision, and F1 score.\n",
    "\n",
    "* Use the bar plot to visulaize the elements of the learned model parameter vector $\\mathbf{w}$. Some elements  have larger absolute values, while the others do not. Try to explain this phenomenon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
